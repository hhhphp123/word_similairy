{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number is pairs is\n",
      "24\n",
      "{('journey', 'car'): 5.85, ('brother', 'monk'): 6.27, ('equipment', 'maker'): 5.91, ('coast', 'hill'): 4.38, ('baby', 'mother'): 7.85, ('canyon', 'landscape'): 7.53, ('doctor', 'personnel'): 5.0, ('soap', 'opera'): 7.94, ('monk', 'slave'): 0.92, ('psychology', 'doctor'): 6.42, ('car', 'automobile'): 8.94, ('type', 'kind'): 8.97, ('psychology', 'health'): 7.23, ('computer', 'laboratory'): 6.78, ('century', 'year'): 7.59, ('coast', 'forest'): 3.15, ('psychology', 'mind'): 7.69, ('school', 'center'): 3.44, ('journey', 'voyage'): 9.29, ('stock', 'egg'): 1.81, ('word', 'similarity'): 4.75, ('professor', 'doctor'): 6.62, ('planet', 'people'): 5.75, ('coast', 'shore'): 9.1}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Mar 23 10:49:48 2018\n",
    "\n",
    "@author: hasee\n",
    "\"\"\"\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "documents = nltk.corpus.brown.paras()\n",
    "file = open( \"combined.tab\")\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def loweCase_lemmatize(document):#pre-process the Brown corpus\n",
    "    lemmaed_document = []\n",
    "    for paragraph in document:\n",
    "        paragraphs = []\n",
    "        for sentence in paragraph:\n",
    "            for word in sentence:\n",
    "                word = lemmatizer.lemmatize(word.lower())\n",
    "                paragraphs.append(word)\n",
    "        lemmaed_document.append(set(paragraphs))\n",
    "    return lemmaed_document\n",
    "\n",
    "def count(word,document):# count the frequnency\n",
    "    count = 0\n",
    "    for paragraph in document:\n",
    "            if word in paragraph:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "tokenized_document = loweCase_lemmatize(documents)\n",
    "\n",
    "def first_filter(document):# first_filter \n",
    "    dictionary = {}\n",
    "    for aRow in file:\n",
    "        list =  aRow.split('\\t')\n",
    "        if count(list[0],document) > 9 and count(list[1],document) > 9:\n",
    "            tuple = (list[0],list[1])\n",
    "            dictionary.setdefault(tuple,list[2])\n",
    "    return dictionary\n",
    "\n",
    "def synset_lemma(word):#find the primary sense\n",
    "    lemmas = {}\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            if lemma.name() == word:\n",
    "                lemmas.setdefault((synset,lemma),lemma.count())\n",
    "    sorted_lemmas = sorted(lemmas.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)\n",
    "    if len(sorted_lemmas) == 1 and sorted_lemmas[0][0][0].pos() == \"n\":\n",
    "        return True,sorted_lemmas[0][0][0]\n",
    "    elif sorted_lemmas[0][1] > 5 and sorted_lemmas[0][1] >= 5*sorted_lemmas[1][1] and sorted_lemmas[0][0][0].pos() == \"n\":\n",
    "        return True,sorted_lemmas[0][0][0] \n",
    "    else:\n",
    "        return False,{}\n",
    "\n",
    "\n",
    "def second_filter(document):#second_filter\n",
    "    new_document = document\n",
    "    keys = new_document.keys()\n",
    "    for key in keys:\n",
    "        if synset_lemma(key[0])[0] == False or synset_lemma(key[1])[0] == False:\n",
    "         del new_document[key]\n",
    "    return new_document\n",
    "\n",
    "def get_primary_senes(pairs):# get the dictionary of word pairs and their primary senses\n",
    "    new_pairs = {}\n",
    "    for key in pairs:\n",
    "        value = [synset_lemma(key[0])[1],synset_lemma(key[1])[1]]\n",
    "        new_pairs.setdefault(key,value)\n",
    "    return new_pairs\n",
    "\n",
    "\n",
    "\n",
    "pairs = second_filter(first_filter(tokenized_document))\n",
    "pairs1 = {}\n",
    "for key in pairs:\n",
    "    pairs1.setdefault(key,float((pairs[key].replace(\"\\n\",\"\"))))\n",
    "    \n",
    "\n",
    "print \"the total number is pairs is\"\n",
    "print len(pairs1)\n",
    "print pairs1\n",
    "\n",
    "pairs_with_primary_senes = get_primary_senes(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('journey', 'car'): 0.09523809523809523, ('brother', 'monk'): 0.5714285714285714, ('equipment', 'maker'): 0.5, ('coast', 'hill'): 0.6666666666666666, ('baby', 'mother'): 0.5, ('canyon', 'landscape'): 0.3333333333333333, ('doctor', 'personnel'): 0.13333333333333333, ('soap', 'opera'): 0.2222222222222222, ('monk', 'slave'): 0.6666666666666666, ('psychology', 'doctor'): 0.1111111111111111, ('car', 'automobile'): 1.0, ('type', 'kind'): 0.9473684210526315, ('psychology', 'health'): 0.21052631578947367, ('computer', 'laboratory'): 0.35294117647058826, ('century', 'year'): 0.8333333333333334, ('coast', 'forest'): 0.16666666666666666, ('psychology', 'mind'): 0.5714285714285714, ('school', 'center'): 0.13333333333333333, ('journey', 'voyage'): 0.8571428571428571, ('stock', 'egg'): 0.11764705882352941, ('word', 'similarity'): 0.3333333333333333, ('professor', 'doctor'): 0.5, ('coast', 'shore'): 0.9090909090909091, ('planet', 'people'): 0.18181818181818182}\n"
     ]
    }
   ],
   "source": [
    "def get_wu_scores(pairs):# get Wu-Palmer scores\n",
    "    pairs_socre = {}\n",
    "    for key in pairs:\n",
    "        value = pairs[key]\n",
    "        synset1 = value[0]\n",
    "        synset2 = value[1]\n",
    "        score = wn.wup_similarity(synset1, synset2)\n",
    "        pairs_socre.setdefault(key,score)\n",
    "    return pairs_socre\n",
    "wu_pairs = get_wu_scores(pairs_with_primary_senes)\n",
    "print wu_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('journey', 'car'): 0, ('brother', 'monk'): 8.588453524677158, ('equipment', 'maker'): 10.19395451157477, ('coast', 'hill'): 6.8415427432365234, ('baby', 'mother'): 8.776192449102656, ('canyon', 'landscape'): 0, ('doctor', 'personnel'): 7.8782012138153705, ('soap', 'opera'): 9.872906914145858, ('monk', 'slave'): 0, ('psychology', 'doctor'): 9.19197535489398, ('car', 'automobile'): 9.178414446597797, ('type', 'kind'): 6.301105661499941, ('psychology', 'health'): 0, ('computer', 'laboratory'): 0, ('century', 'year'): 6.506664771776898, ('coast', 'forest'): 8.713787196038151, ('psychology', 'mind'): 8.403103611436123, ('school', 'center'): 6.4280926584302005, ('journey', 'voyage'): 0, ('stock', 'egg'): 7.474479027764361, ('word', 'similarity'): 0, ('professor', 'doctor'): 0, ('planet', 'people'): 6.028474169984525, ('coast', 'shore'): 10.394796384230341}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def get_PPMI(word1,word2,document):# get the ppmi of 2 words\n",
    "    word1_count = 0\n",
    "    word2_count = 0\n",
    "    both_count = 0\n",
    "    total_count = 0.0\n",
    "    for paragraph in document:\n",
    "        total_count += len(paragraph)\n",
    "        if word1 in paragraph:\n",
    "            word1_count += 1\n",
    "            if word2 in paragraph:\n",
    "                both_count += 1\n",
    "        elif word2 in paragraph:\n",
    "            word2_count += 1\n",
    "    if both_count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        pmi = math.log((both_count/total_count)/((word1_count/total_count)*(word2_count/total_count)), 2)\n",
    "        return max(pmi,0)\n",
    "\n",
    "\n",
    "def get_PPMI_pair(pair,document):#get the dictionary of words pairs and their PPMI\n",
    "    new_pairs = {}\n",
    "    for key in pairs:\n",
    "        value = get_PPMI(key[0],key[1],document)\n",
    "        new_pairs.setdefault(key,value)\n",
    "    return new_pairs\n",
    "PPMI_pairs = get_PPMI_pair(pairs,tokenized_document)\n",
    "print PPMI_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('journey', 'car'): -0.015060280564988648, ('brother', 'monk'): 0.07901775098870045, ('equipment', 'maker'): 0.2885449049623524, ('coast', 'hill'): 0.1841347061981673, ('baby', 'mother'): 0.29760477229827775, ('canyon', 'landscape'): 0.09013870783128552, ('doctor', 'personnel'): 0.05418676478243645, ('soap', 'opera'): 0.014608558896880952, ('monk', 'slave'): -0.01587697235481622, ('psychology', 'doctor'): 0.06400045349324868, ('car', 'automobile'): 0.32473844604925794, ('type', 'kind'): 0.023739016763977716, ('psychology', 'health'): 0.04362310736055919, ('computer', 'laboratory'): 0.12677766307459515, ('century', 'year'): 0.06258368801916647, ('coast', 'forest'): 0.11656406706199751, ('psychology', 'mind'): 0.11774001276758295, ('school', 'center'): 0.04220126640079802, ('journey', 'voyage'): 0.12643390494882167, ('stock', 'egg'): 0.09995085202884646, ('word', 'similarity'): 0.0030614516447731166, ('professor', 'doctor'): 0.05309113583106195, ('planet', 'people'): 0.025506413220330426, ('coast', 'shore'): 0.42059683618139454}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "texts = []\n",
    "for paragraph in tokenized_document:\n",
    "    texts.append(get_BOW(paragraph))   \n",
    "    \n",
    "vectorizer = DictVectorizer()\n",
    "matrix = vectorizer.fit_transform(texts)\n",
    "matrix = np.transpose(matrix)\n",
    "transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "matrix = transformer.fit_transform(matrix)\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "matrix = svd.fit_transform(matrix)\n",
    "\n",
    "def get_row(word):\n",
    "    return vectorizer.get_feature_names().index(word)\n",
    "\n",
    "def svd_similarity_pairs(pairs,matrix):\n",
    "    new_pairs = {}\n",
    "    for key in pairs:\n",
    "        vectoe1  = matrix[get_row(key[0])]\n",
    "        vectoe2  = matrix[get_row(key[1])]\n",
    "        value = 1 - cos_distance(vectoe1,vectoe2)\n",
    "        new_pairs.setdefault(key,value)\n",
    "    return new_pairs\n",
    "    \n",
    "svd_pairs = svd_similarity_pairs(pairs,matrix)\n",
    "print svd_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "sentences = nltk.corpus.brown.sents()\n",
    "def train(text):\n",
    "    model = gensim.models.Word2Vec(text,size = 500,iter = 50)\n",
    "    return model\n",
    "\n",
    "model = train(sentences)\n",
    "    \n",
    "def get_Word2vec_pair(pair,model):\n",
    "    new_pairs = {}\n",
    "    for key in pairs:\n",
    "        value = model.wv.similarity(key[0],key[1])\n",
    "        new_pairs.setdefault(key,value)\n",
    "    return new_pairs\n",
    "\n",
    "word2vec_pairs = get_Word2vec_pair(pairs,model)\n",
    "print word2vec_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def dic_to_list(dic):\n",
    "    return dic.items()\n",
    "\n",
    "dataset = [word2vec_pairs,svd_pairs,PPMI_pairs,wu_pairs]\n",
    "setname = [\"word2vec_pairs\",\"svd_pairs\",\"PPMI_pairs\",\"wu_pairs\"]\n",
    "\n",
    "\n",
    "\n",
    "def co_efficient(dataset,setname):\n",
    "    for i in range(len(dataset)):     \n",
    "            set1 = dataset[i].values()\n",
    "            pearsonr = stats.pearsonr(set1,pairs1.values())\n",
    "            print \"pearsonr for set \"+ setname[i]+\" and gold standardis \" + str(pearsonr)\n",
    "            \n",
    "print co_efficient(dataset,setname)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
